% small.tex
\documentclass{beamer}
\usepackage{amsmath,amssymb,amsthm,color}
\usepackage{movie15}
\usetheme{Madrid}

%\usepackage[pdftex]{graphicx}
%\DeclareGraphicsExtensions{.pdf,.mps,.png,.jpg}
\usepackage{subfig}
\usepackage{algorithm,algpseudocode}
\newcommand{\R}{\mathbb{R}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Sph}{\mathbb{S}}

\newcommand{\delr}{\dfrac{r}{\delta}}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\ep}{\varepsilon}
\newcommand{\epz}{\ep\longrightarrow 0}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vf}{\vect{f}}
\newcommand{\vg}{\vect{g}}
\newcommand{\vx}{\vect{x}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vxi}{\boldsymbol{\xi}}
\newcommand{\vP}{\vect{P}}
\newcommand{\vp}{\vect{p}}
\newcommand{\vH}{\vect{H}}
\newcommand{\vI}{\vect{I}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vL}{\vect{L}}
\newcommand{\vn}{\vect{n}}
\newcommand{\vu}{\vect{u}}
\newcommand{\bfe}{\vect{e}}
\newcommand{\coef}{c}
\newcommand{\uu}{u_X}
\newcommand{\dt}{\Delta t}

\newcommand{\lam}{\lambda}
%\newcommand{\laps}{\Delta_{\mathcal{S}}}
%\newcommand{\Mlap}{\Delta_{\mathcal{S}}}
%\newcommand{\Mgrad}{\nabla_{\mathcal{S}}}
\newcommand{\laps}{\Delta_{\M}}
\newcommand{\Mlap}{\Delta_{\M}}
\newcommand{\Mgrad}{\nabla_{\M}}
\newcommand{\grad}{\nabla}
\newcommand{\MEgrad}{\vP\nabla}
\newcommand{\Mdiv}{\Mgrad\cdot}
\newcommand{\pder}[1]{\dfrac{\partial}{\partial#1}}
\newcommand{\pgradx}{\mathcal{G}^{x}}
\newcommand{\pgrady}{\mathcal{G}^{y}}
\newcommand{\pgradz}{\mathcal{G}^{z}}

\newcommand{\comment}[1]{{\color{red}{#1}}}

%Kevin's commands

\newcommand{\supp}{\operatorname{supp}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\plotwidth}{0.4}
\newcommand{\conwidth}{0.45}
\newcommand{\diffOp}{\mathcal{L}}
\newcommand{\slfrac}[2]{\left.#1\middle/#2\right.}
\DeclareMathOperator{\sech}{sech}
\newcommand{\f}{\underline{f}}
\newcommand{\vcoefs}{\underline{c}}
\newcommand{\x}{\underline{x}}
\newcommand{\y}{\underline{y}}
\newcommand{\z}{\underline{z}}

\DeclareMathOperator*{\osc}{osc}

% items enclosed in square brackets are optional; explanation below
\title[What I've done]{What I am doing}
\author[K. Aiton]{Kevin Aiton}
\institute[UD]{
  Department of Mathematics\\
University of Delaware\\
}
\date[November 2017]{May 10, 2018}


\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% TITLE PAGE
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain]
  \titlepage
\end{frame}

\begin{frame}{Additive Schwarz}

		We are currently working on an adaptive \textbf{additive Schwarz} method for solving boundary value problems. To solve $\mathcal{L}(u)=f$ in $\Omega$, $u=g$ on $\partial \Omega$:
\bigskip


	\begin{center}
	for $n=1,2,\dots$ solve
	\begin{equation*}
	\begin{aligned}
	&\mathcal{L}(u_1^{n+1})=f \text{ in } \Omega_1\\
	&u_1^{n+1} = g \text{ at } \partial\Omega_1 \setminus \Gamma_1 \\
	&u_1^{n+1} = u_2^{n} \text{ at } \Gamma_1	\end{aligned} \quad 	\begin{aligned}
	&\mathcal{L}(u_2^{n+1})=f \text{ in } \Omega_2 \\
	&u_2^{n+1} = g \text{ at } \partial\Omega_2 \setminus \Gamma_2  \\
	&u_2^{n+1} = u_1^{n} \text{ at } \Gamma_2 \end{aligned}
	\end{equation*}	
	\end{center}

\begin{center}
		\includegraphics[scale  = 0.4]{AS.eps}
\end{center}


\end{frame}

\begin{frame}{Additive Schwarz}
		To discretely solve $\mathcal{L}(u)=f$ in $\Omega$, $u=g$ on $\partial \Omega$ with Chebyshev polynomials we create discrete operators $A_1$,$A_2$ and then for $n=1,2,\dots$ solve
		\begin{center}
	\begin{equation*}
A_1 u_1^{n+1} =  \begin{bmatrix} 
  f\rvert_{\Omega_1} \\ g\rvert_{\Omega_1 \setminus \Gamma_1} \\ u_2^{n} \rvert_{\Gamma_1}
  \end{bmatrix}, \quad A_2 u_2^{n+1} =  \begin{bmatrix} 
  f\rvert_{\Omega_2} \\ g\rvert_{\Omega_2 \setminus \Gamma_2} \\ u_1^{n} \rvert_{\Gamma_2}
  \end{bmatrix}
	\end{equation*}
	\end{center}

\begin{center}
		\includegraphics[scale  = 0.4]{AS.eps}
\end{center}


\end{frame}


\begin{frame}{Additive Schwarz}
		Define matrix $B_{12}$ to map the polynomial $u_2$ to $u_2 \rvert_{\Gamma_1}$, and define $B_{21}$ similarly. Let $u^n = \begin{bmatrix}
			u_1^n \\ u_2^n
		\end{bmatrix}$. Additive Schwarz can be solved in block form like so:
		\begin{center}
		for $n=1,2,\cdots$ solve
	\begin{equation*}
\underbrace{\begin{bmatrix}
A_1 & 0 \\
0 & A_2	
\end{bmatrix}}_{A} u^{n+1} = \underbrace{\begin{bmatrix}
0 & B_{12} \\
B_{21} & 0	
\end{bmatrix}}_{B} u^n + \underbrace{\begin{bmatrix}
	f\rvert_{\Omega_1} \\ g\rvert_{\Omega_1 \setminus \Gamma_1} \\ 0 \\ f\rvert_{\Omega_2} \\ g\rvert_{\Omega_2 \setminus \Gamma_2} \\ 0
\end{bmatrix}}_{b}
	\end{equation*}
	\end{center}

\end{frame}

\begin{frame}{Additive Schwarz}
		Adding and subtracting $u^n$	, we see that	
		\begin{center}
	\begin{equation*}
	u^{n+1} = u^n + A^{-1}((B-A)u^n+b)	
	\end{equation*}
	We then recognize:
	\begin{itemize}
		\item if $u^0=0$, then
\begin{equation*}
\begin{aligned} 
	& u^1 = A^{-1} b \\
	& u^2 = 2 A^{-1}b +A^{-1}(B-A)A^{-1}b \\
	& \vdots
\end{aligned}
\end{equation*}
That is, $u^n$ is in the Krylov subspace of $\mathcal{K}_n(A^{-1}(A-B), A^{-1}b)$.
		\item If $\| A^{-1}(B-A)u^n+b \|$ is small then $\| u^{n+1} - u^{n} \|$ is small as well.
	\end{itemize}
	\end{center}
\end{frame}

\begin{frame}{Additive Schwarz}
\begin{itemize}
		\item \textbf{GMRES} approximates $A^{-1}(B-A)x = A^{-1} b$ with $x_n \in \mathcal{K}_n(A^{-1}(A-B), A^{-1}b)$ such that $x_n$ minimizes $\| A^{-1}(B-A)x_n - A^{-1} b \|$.
		\item We can thus accelerate Additive Schwarz by using GMRES to minimize $\|(B-A)u-b\|$ with $A^{-1}$ as a preconditioner.
\end{itemize}
\end{frame}

\begin{frame}{Additive Schwarz}
We use a two-level iteration that allows rapid communication between distant domains. Defining block course matrices $A_c$, $B_c$, coarsening operator $C$, and refinement operator $P$ we define a preconditioner $M:b\rightarrow v $ such that

\begin{equation*}
\begin{aligned}
	v &\leftarrow P (A_c-B_c)^{-1} C b \\
	v &\leftarrow v + A^{-1}(b-Av).
\end{aligned}
\end{equation*}
\end{frame}

\begin{frame}{Example}

Here we adaptively solve
$$
\varepsilon \Delta u + 2 u_x+u_y = f(x,y) \text{ in } \Omega, \quad u=b(x,y) \text{ on } \partial\Omega
$$
\bigskip
where $\varepsilon=5\times10^{-3}$ and $u(x,y) = \left(1-e^{\frac{x-1}{\epsilon }}\right) \left(1-e^{\frac{y-1}{\epsilon }}\right) \cos (\pi  (x+y)).$
\begin{columns}[t]

\begin{column}{0.5\textwidth}	
\begin{center}
\includegraphics[scale = 0.3]{coolPDE.eps}	
\end{center}
\end{column}

\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[scale = 0.3]{coolPDEdomain.eps}	
\end{center}
\end{column}

\end{columns}

\begin{center}
For this solution we adapted to tol=$10^{-8}$, and got error $5 \times 10^{-8}$.
\end{center}

\end{frame}

\begin{frame}{Current work}
\begin{itemize}
\item We can apply the current to the blinking eye model using the conformal map.
\item We can solve the blinking eye problem by using Additive Schwarz with least square approximations of the eye.
\end{itemize}

\begin{columns}[c]

\begin{column}{0.5\textwidth}	
\begin{center}
\includegraphics[scale = 0.3]{LS_3D.eps}	

$\Omega$

\end{center}
\end{column}

\begin{column}{0.5\textwidth}
For a function $f:\Omega \to \R$ and a set of points $\Xi \in \Omega$, we find a polynomial $p$ in the span of a Chebyshev polynomial basis that minimizes the least square problem $$ \sum_{x_i \in \Xi} (f(\vect{x_i})-p(\vect{x_i}))^2. $$
\end{column}
\end{columns}
\end{frame}


\end{document}