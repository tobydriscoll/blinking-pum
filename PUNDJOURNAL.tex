% SIAM Article Template
\documentclass{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfig}
\usepackage{forest}
\usepackage[section]{placeins}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\usepackage{amsthm}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand\supp{\mathop{\rm supp}}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\plotwidth}{0.45}
\newcommand{\WRP}{\par\qquad\(\hookrightarrow\)\enspace}

%%%%% custom commands for this paper
\newcommand{\nmax}{n_{\text{max}}}
\newcommand{\child}[1]{c\textsubscript{#1}}
\newcommand{\weight}[1]{w\textsubscript{#1}}
\newcommand{\bound}[1]{b\textsubscript{#1}}
%%%%%

\begin{document}

\section{ND PU Structure}

Since we are thinking of mixing methods for our PU approximation, it seems we should think more about the modularity of the method. My first thought is to have an abstract PATCH $\nu$ with properties
\begin{itemize}
\item domain($\nu$):= domain of $\nu$
\item boundaryIndex($\nu$):= boundary index of points of $\nu$
\item length($\nu$):= length of $\nu$
\item dim($\nu$):= dimension of the space domain($\nu$) lies in
\end{itemize}
with methods
\begin{itemize}
\item points($\nu$):=returns the points of $\nu$
\item evalf($\nu$,x):=evaluates the approximate for $\nu$ at $x$.
\item sample($\nu$,f(x)):= samples $f(x)$ at the leaves of $\nu$
\end{itemize}

\begin{figure}[!h]
\centering
     \begin{forest}
[ PATCH
    [ LEAFPATCH 
    [ RBFPATCH
    ] 
    [ CHEBPATCH
    ]]
    [ PUPATCH 
    [ PURECTPATCH ]
    ]
]
\end{forest}
\caption{Object structure for the patches.}
\end{figure}
Here RBFPATCH and CHEBPATCH would be used as leaves where RECTPUPATCH would be used for nonleaves. A PUPATCH has additional properties
\begin{itemize}
\item \child{0}($\nu$), \child{1}($\nu$):= two PATCH children
\item \weight{0}($\nu$), \weight{1}($\nu$):= weights used for children
\end{itemize}
The methods would be defined, ignorant of the type of patches the children are. As an example, the evalf($\nu$,x) method would be defined similarly as before where the leaves are assumed to be type PATCH (this objects would have there own evalf method). By defining different objects for different methods (RBF,CHEB), we can use this generic method for our different cases (a box, arbitrary boundary).

 The leaf patches would be defined for the specific method at the leaves. We next define a method 
\begin{itemize}
\item PATCH $\nu_1$ = splitleaf(LEAFPATCH $\nu$)
\end{itemize}
that returns a PUPATCH with two children if $\nu$ needs to be split, and $\nu$ if it does not to be split. We can then define a refine method for a tree $T$ and its nodes $\nu$.

\begin{algorithm}[!h]
\caption{v=eval($\nu$,$x$)}
\label{alg3}
\begin{algorithmic}
\IF{$\nu$ is a leaf}
\STATE $p$:=interpolant($\nu$)
\STATE v:= $p(x)$
\ELSE
\STATE $v_0,v_1$:=0

\STATE $w_0$:=\weight{0}($\nu$)
\STATE $w_1$:=\weight{1}($\nu$)
\FOR{$k=0,1$}
\IF{$x \in$ domain(\child{k}($\nu$))}
\STATE $v_k$:=eval(\child{k}($\nu$),$x$)
\ENDIF
\ENDFOR
\STATE v := $w_0(x)v_0 + w_1(x)v_1$
\ENDIF
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[!h]
\caption{T = refine($f(x)$)}
\label{alg5}
\begin{algorithmic}
\STATE define $T$ has a tree with single node
\WHILE{$T$ has unrefined leaves}
\STATE sample($T$,$f(x)$)
\IF{root($T$) is a leaf}
\STATE root($T$) = splitleaf(root($T$))
\ELSE
\STATE PUsplit(root($T$))
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!h]
\caption{PUsplit($\nu$)}
\label{alg6}
\begin{algorithmic}
\FOR{$k=0,1$}
\IF{\child{k}($\nu$) is a leaf and unresolved}
\STATE \child{k}($\nu$):=splitleaf(\child{k}($\nu$))
\ELSE
\STATE PUsplit(\child{k}($\nu$))
\ENDIF
\ENDFOR
\STATE merge($\nu$)
\end{algorithmic}
\end{algorithm}

\section{Calculating Derivatives and Partition of Unity Weights}

For the ND problem, we intend to evaluate the derivatives instead of constructing differentiation matrices. Suppose we have a method evalf($\nu$,$x$,$dim$,$j$) which evaluates the derivative of order $j$ along dimension $dim$ at $x$. We can compute this simply using the product rule as seen in Algorithm~\ref{alg7}. While this is formulation is simple, it is inefficient. In this formulation we end up repeating calculations for the derivatives of the children of the node. This can be avoided by passing up the evaluations of the approximation and its derivatives up to the desired order.

\begin{algorithm}[!h]
\caption{v = evalf($\nu$,$x$,$dim$,$j$)}
\label{alg7}
\begin{algorithmic}
\STATE $w_0$:=\weight{0}($\nu$)
\STATE $w_1$:=\weight{1}($\nu$)
\STATE v = 0
\FOR{$k=0,1$}
\STATE v = v+$\sum_{i=1}^j \binom{j}{i} \partial_{dim}^{j-1} w_k(x) \text{ evalf(\child{k}($\nu$),$x$,$dim$,$j$)}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

For our method we intend to split only along one dimension. In this case our partition of unity weights need only to depend on the dimension that we are splitting along. Some care must be used when calculating the derivatives; if the dimension we are differentiation along is different than the splitting dimension, the derivative of the weights are zero.

\section{Barycentric Interpolation on ND Chebyshev grids}

Suppose we have a tensor product of Chebyshev grids with points $\{x_i\},\{y_j\}$. Let $\{l_{x_i}(x)\}$, $\{l_{y_i}(x)\}$ be the Lagrange polynomials for points $\{x_i\},\{y_j\}$. We then have the interpolating polynomial for a function $f(x,y)$ is
\begin{equation}
p(x,y) = \sum_j \sum_i l_{y_j}(y) l_{x_i}(x) f(x_i,y_j)
\end{equation}
which can be expressed as
\begin{equation}
p(x,y) = \sum_j l_{y_j}(y) \sum_i l_{x_i}(x) f(x_i,y_j).
\end{equation}
This implies that we can evaluate p(x,y) by:
\begin{itemize}
\item Evaluating $c_j = \sum_i l_{x_i}(x) f(x_i,y_j)$ for all points $\{y_j\}$,
\item and then evaluating $\sum_j l_{y_j}(y) c_j$
\end{itemize}
We can thus evaluate $p(x,y)$ with interpolation methods in one dimension. This is illustrated in Figure~\ref{MULTI_INTERP}.

Chebfun includes a \mcode{bary(x,F)} method which will evaluate the Chebyshev interpolants at \mcode{x} given a sampling \mcode{F}; if \mcode{F} is matrix, the method will compute the Chebyshev interpolants for each of the columns of \mcode{F}. The method can thus be used as is for 2D interpolation. But lets suppose \mcode{F} is $f(x,y,z)$ sampled on a 3D Chebyshev tensor product grid of dimension $n_1 \times n_2 \times n_3$ (i.e., $F$ is a multidimensional array). Lets now suppose we have $\{x_i\},\{y_j\}, \{z_k\}$. Let $\{l_{x_i}(x)\}$, $\{l_{y_i}(x)\},\{l_{z_k}(x)\}$ be the Lagrange polynomials for points $\{x_i\},\{y_j\},\{z_k\}$. Then in-order to evaluate
\begin{equation}
p(x,y,z) = \sum_k l_{z_k}(z) \sum_j l_{y_j}(y)  \sum_i  l_{x_i}(x) f(x_i,y_j,z_k)
\end{equation}
by
\begin{itemize}
\item Evaluating $c_{kj} = \sum_i  l_{x_i}(x) f(x_i,y_j,z_k)$ for points $\{y_j\}, \{z_k\}$,
\item evaluating $b_{k} = \sum_j l_{y_j}(y) c_{kj}$ for points $\{z_k\}$
\item and then evaluating $\sum_k l_{z_k}(z) b_k$.
\end{itemize}
To do this first step, I call \mcode{bary(x(1),reshape(F,[n1 n2*n3]))}; by reshaping \mcode{F}, I can evaluate the Chebyshev polynomials that run along the $x$-dimension. This can be generalized to higher dimensions. I rewrote the \mcode{bary} method to accept multidimensional arrays for \mcode{F} and interpolate along the first dimension.
 
Lets suppose we want to interpolate onto a tensor product grid with points $\{\hat{x}_i\}$, $\{\hat{y}_j\}$, $\{\hat{z}_k\}$, and lets suppose for simplicity Chebyshev grid has $N$ points in each direction while the interpolating set of points has $M$ points in each direction.

 There are two ways to evaluate $p(x,y,z)$ on the grid. The first is to use method above for each of the points. This results in a cost of
 \begin{equation}
 \mathcal{O}(M^3 N^3 + M^3 N^2 + M^3 N).
 \end{equation}
Let suppose instead that we calculate the p(x,y) along the $x$-dimension for $\{\hat{x}_i\}$, i.e. call \mcode{G=bary(x,F)}, where \mcode{x} is the vector of points $\{\hat{x}_i\}$ (with my rewritten code). Here \mcode{G} will be a multidimensional array of dimension $M \times N \times N$. In this case, \mcode{G(i,:,:)} is the set of Chebyshev interpolant evaluations for points $\{y_j\}, \{z_k\}$ calculated at $\hat{x}_i$.

Let's shift the dimensions of \mcode{G} with \mcode{G = shiftdim(G,1)} i.e. now the dimensions of G are $N \times N \times M$. Since we are interpolating on a grid, for each $\hat{y}_j$ we need to evaluate $p(\hat{x}_i,y,z)$ for each $i$. This can be achieved by calling \mcode{G=bary(y,G)}, where \mcode{y} is the vector of points $\{\hat{y}_j\}$. This is repeated for the $z$-dimension. Putting it altogether, we have
\begin{lstlisting}[frame=single]  % Start your code-block

G=bary(x,F);
G = shiftdim(G,1);
G=bary(y,G);
G = shiftdim(G,1);
G=bary(z,G);
G = shiftdim(G,1);
\end{lstlisting}
Our final result will be a $M \times M \times M$ grid evaluated at the interpolating tensor product grid. The work required is
 \begin{equation}
 \mathcal{O}(M N^3 + M^2 N^2 + M^3 N).
 \end{equation}
 Thus if $N =\mathcal{O}(M)$ we have the first method requires $\mathcal{O}(M^6)$ work, while the second required $\mathcal{O}(M^4)$. This can be seen computationally. In my experiments, I simulate a possible splitting. Assuming a max degree 65 in all dimensions, the first method requires 15 seconds to interpolate onto the grid. The second require 0.18 seconds. The code I used is below.
 
\begin{lstlisting}[frame=single]  % Start your code-block

clear;

domain = [-1 1];

standard_degs = [3 5 9 17 33 65];

deg_ind = [5 5 5];
degs = standard_degs(deg_ind);

x=chebpts(degs(1),domain);
y=chebpts(degs(2),domain);
z=chebpts(degs(3),domain);


M = 6;
N=3;
chebpoints = cell(M,1);

chebmatrices = cell(M,2);

chebweights = cell(M,1);



for i=1:M
    chebpoints{i} = chebpts(N);
    chebmatrices{i,1} = diffmat(N,1);
    chebmatrices{i,2} = diffmat(N,2);
    chebweights{i} = chebtech2.barywts(N);
    N = N+(N-1);
end

numb = 65;

%Simulate a splitting
xc = linspace(-1,1,65)';
yc = linspace(-1,1,65)';
yc = yc(yc>1-0.75); 
zc = linspace(-1,1,65)';

grid_points = {xc,yc,zc,wc};

[X3C,Y3C,Z3C] = ndgrid(xc,yc,zc);


XP3 = [X3C(:) Y3C(:) Z3C(:)];


[X3,Y3,Z3] = ndgrid(x,y,z);

tic;
G = F3;

h = @(x) 2/(domain(2)-domain(1))*x-(domain(2)+domain(1))/(domain(2)-domain(1));

for k=1:ndims(X3C)
    G = bary(h(grid_points{k}),G,chebpoints{deg_ind(k)},chebweights{deg_ind(k)});
    G = shiftdim(G,1);
end
toc

F3C = X3C.^2+Y3C.*X3C+Z3C.^3;

max(abs(F3C(:)-G(:)))

FUNS = zeros(length(XP3),1);

tic;
for i=1:size(XP3,1)
    G = F3;
    for k=1:size(XP3,2)
        G = bary(XP3(i,k),G,chebpoints{deg_ind(k)},chebweights{deg_ind(k)});
    end
    FUNS(i) = G;
end
toc

max(abs(FUNS(:)-F3C(:)))
;
\end{lstlisting}

 
\begin{figure}[!htb]
\centering
\subfloat[Chebyshev tensor product grid (in blue), and point to be evaluated (in black).]{
\includegraphics[scale = 0.5]{grid1.eps}
   \label{MERGEB}
 }\hfill
\subfloat[Chebyshev polynomials evaluated along the x-axis at the red points.]{
\includegraphics[scale = 0.5]{grid2.eps}
   \label{MERGEA}
 }
 
 \subfloat[Chebyshev polynomial evaluated along at the desired point.]{
\includegraphics[scale = 0.5]{grid3.eps}
   \label{MERGEA}
 }
\caption{These pictures illustrate how we can computed multivariate Chebyshev interpolants using a one dimensional interpolation method.
}
\label{MULTI_INTERP}
\end{figure}

\section{Some Initial Results}
I have tested the our method on a  2D box with domain $[-1,1]x[-1,1]$. For the function
\begin{equation}
\arctan((x+y)/0.05),
\end{equation}
we have that our method takes 2.135446 seconds (with a splitting tolerance of 1e-14) while Chebfun2 takes 18 seconds.

For the function
\begin{equation}
\arctan(x/0.05)+\arctan(y/0.05),
\end{equation}
our method takes 1.4 seconds while Chebfun2 takes 0.1 seconds. Here is my initial though: Our method's complexity depends on how sharp the features of the function are while Chebfun2's complexity depends on the rank of the function.

\section{RASPEN Solver}
Here I lay out a plan for our to approach the Raspin Solver for a 1D BVP. Here, \bound{0}($\nu$),\bound{1}($\nu$) refer to the left and right interval points of interval($\nu$) as in our paper. The localsolve($\nu$,lbc,rbc) method solves the BVP locally on $\nu$ with the given boundary conditions; this is a method for leaves. The AlternatingSchwartz method would be called on non-leaves. This algorithm can be generalized to higher dimensions, but some care must be given to the boundaries. I will leave this for later.

\begin{algorithm}[!h]
\caption{v=AlternatingSchwartz($\nu$,lbc,rbc)}
\label{alg8}
\begin{algorithmic}
\IF{\child{0}($\nu$) is not a leaf}
\STATE $\hat{\text{rbc}}$ = evalf(\child{1}($\nu$),\bound{1}(\child{0}($\nu$)))
\STATE v0 = AlternatingSchwartz(\child{0}($\nu$),lbc,$\hat{\text{rbc}}$)
\ELSE
\STATE v0 = localsolve(\child{0}($\nu$),lbc,rbc)
\ENDIF

\IF{\child{1}($\nu$) is not a leaf}
\STATE $\hat{\text{lbc}}$ = evalf(\child{0}($\nu$),\bound{0}(\child{1}($\nu$)))
\STATE v1 = AlternatingSchwartz(\child{1}($\nu$),$\hat{\text{lbc}}$,rbc)
\ELSE
\STATE v1 = localsolve(\child{1}($\nu$),lbc,rbc)
\ENDIF
\STATE v = [v0;v1];
\end{algorithmic}
\end{algorithm}

For Algorithm~\ref{alg9}, the residuals of the BVP'S should be included in the nonlinear solve.

\begin{algorithm}[!h]
\caption{c=CourseSolve(T,$x_c$,$D_x$,$D_{xx}$,$u$)}
\label{alg9}
\begin{algorithmic}
\STATE sample(T,$u$)
\STATE [ubglob,udglobdx,uglobd2x] = evalf(T,points(T))
\STATE rglob = ODE(ubglob,udglobdx,uglobd2x)
\STATE $u_c$ = evalf(T,$x_c$)
\STATE $r_c$ = ODE($u_c$,$D_x u_c$, $D_{xx} u_c$)
\STATE sample(T,rglob)
\STATE g = rc + evalf(T,$x_c$)
\STATE c = fsolve(@(s) ODE($s$,$D_x s$,$D_{xx} s$)-g)-$u_c$
\end{algorithmic}
\end{algorithm}

We finally have the RASPEN iteration in \ref{alg10}.
\begin{algorithm}[!h]
\caption{F=Raspen($u$,T,$x_c$,$D_x$,$D_{xx}$,lbc,rbc)}
\label{alg10}
\begin{algorithmic}
\STATE $u_{\text{init}}=u$
\STATE cf=Chebfun(CourseSolve(T,$x_c$,$D_x$,$D_{xx}$,$u$))
\STATE sample(T,u+cf(points(T)))
\STATE Alt = AlternatingSchwartz(T,lbc,rbc)
\STATE sample(T,Alt)
\STATE F = unit-evalf(T,points(T))
\end{algorithmic}
\end{algorithm}

\section{Computing on a Gird}
I completed the code for evaluating the approximation on the grid; this works very fast. The dominate costs for the approximation evaluation on a grid $X$ are:
\begin{itemize}
\item Determining the sub-grids of $X$ are in the nodes of the tree,
\item and calculating the weights.
\end{itemize}
For a tree $T$ with leaves $\{\nu_i\}_{i=1}^N$, let grid($\nu_i$) be the grid of the leaf (in Matlab, this would be stored as a cell array of coordinate vectors). For a forward solver, we would need to evaluate the approximation on $\{\text{grid($\nu_i$)}\}_{i=1}^N$. From what I have seen, a single iteration in the RASPEN solve will require hundreds of evaluations. For the approximation of $tan(x+y)$, the method took 6 seconds to evaluate on the grid.

Looking at the profiler though, almost \%75 to \%80 of the work is determining which points belong to which patches and calculating the weights. These can be pre-calculated. Suppose we have a cell array of the leafs as well as the tree (since I am using the handle class, changes in one data structure will change the other since Matlab uses references).

First, we could use the tree to determine $\text{leafpoints($\nu$)}=\text{points($T$)}\cap\text{domain($\nu$)}$. In this case, $\text{leafpoints($\nu$)}$ could be stored as a cell array of grids. Let's look at an example of a tree in Figure~\ref{Tree_weights} to see how we might precalculate the weights. In this case, the approximate in terms of the leaves is
\begin{equation}
s_{[a,b]}(x) =  w_{\ell_1}(x) s_{[a_{1},b_{1}]}(x) +  w_{r_1}(x)  w_{\ell_2}(x) s_{[a_{21},b_{21}]}(x) + w_{r_1}(x) w_{r_2}(x) s_{[a_{22},b_{22}]}(x)
\end{equation}
In this case we would pre-calculate
\begin{equation}
\begin{aligned}
w_{\ell_1}(x) &\text{ at leafpoints($\nu_1$),} \\
w_{r_1}(x)  w_{\ell_2}(x) &\text{ at leafpoints($\nu_2$),} \\
\text{and } w_{r_1}(x) w_{r_2}(x)  &\text{ at leafpoints($\nu_3$).}
\end{aligned}
\end{equation}

For a general tree, we can write a recursive function to do this as seen in Algorithm~\ref{alg11}. For a node $\nu$, let $\text{weight($\nu$)}$ be the weight multiplied by the approximate for the node (i.e., in Figure~\ref{Tree_weights} $\text{weight($\nu_1$)}=w_{\ell_2}(x)$). For the root of the tree, we set the weight to the constant function 1. In my code I use a standard weight with parameters to shift and scale; this implies that we can just store the parameters for the weight used at the node of the tree. For each leaf $\nu_i$ of the tree $T$, we set
\begin{equation}
\text{weightvals($\nu_i$)=CalculateWeights(root($T$),domain($\nu_i$),leafpoints($\nu_i$))}.
\end{equation}
Let leafpointindex($\nu_i$) be the indices of the points in leafpoints(root($T$)). I describe in Algorithm~\ref{alg12} how to evaluate the approximation on the grids of the tree. This can easily be implemented using a parfor loop if needed.

\begin{algorithm}[!h]
\caption{w=CalculateWeights($\nu$,dom,$X$)}
\label{alg11}
\begin{algorithmic}
\IF{$\nu$ is a leaf}
\STATE $\text{w} \leftarrow \left . \text{weights($\nu$)} \right |_{X}$
\ELSIF{$\text{dom}\subseteq \text{domain(\child{0}($\nu$))}$}
\STATE $\text{w} \leftarrow \left . \text{weights($\nu$)} \right |_{X}.*\text{CalculateWeights(\child{0}($\nu$),dom,$X$)}$
\ELSE
\STATE $\text{w} \leftarrow \left . \text{weights($\nu$)} \right |_{X}.*\text{CalculateWeights(\child{1}($\nu$),dom,$X$)}$
\ENDIF
\end{algorithmic}
\end{algorithm}



\begin{algorithm}[!h]
\caption{F=evalfTreeGrid($T$)}
\label{alg12}
\begin{algorithmic}
\FOR{ each leaf $\nu_i$ of $T$}
\STATE $F_i = \text{zeros}(\text{length}(T),1)$
\STATE $F_i(\text{leafpointindex($\nu_i$)}) \leftarrow \text{weightvals($\nu_i$)}.*\text{evalf}(\nu_i,\text{leafpoints($\nu_i$)})$
\ENDFOR
\STATE $F = \sum_{i=1}^N F_i$.
\end{algorithmic}
\end{algorithm}

\begin{figure}
\centering
     \begin{forest}
for tree={circle,draw, l sep=20pt,,scale=0.98}
[ {$\begin{array}{c}
[a,b] \\
s_{[a,b]}(x)
\end{array}$}
    [ {$\begin{array}{c}
 [a_1,b_1]  \\
s_{[a_{1},b_{1}]}(x) \\
 w_{\ell_1}(x) \\
 \nu_1
\end{array}$},name=left_p]
    [ {{$\begin{array}{c}
    [a_2,b_2] \\
s_{[a_{2},b_{2}]}(x) \\
 w_{r_1}(x) 
\end{array}$}} 
      [{$\begin{array}{c}
 [a_{21},b_{21}] \\     
s_{[a_{21},b_{21}]}(x) \\
 w_{\ell_2}(x) \\
 \nu_2
\end{array}$},name=right_p] 
      [{$\begin{array}{c}
  [a_{22},b_{22}] \\    
s_{[a_{22},b_{22}]}(x) \\
 w_{r_2}(x) \\
  \nu_3
\end{array}$}] 
  ] 
]
]
\end{forest}
\caption{An example of a simple tree with nodes ${\nu_1,\nu_2,\nu_3}$, where each node is labeled with its domain, PU approximate and weight (in that order).}
\label{Tree_weights}
\end{figure}

\section{Over determined least squares method}
Suppose we have an orthogonal set of tensor product chebyshev polynomials $\{ T_k(x,y) \}_{k=1}^{\infty}$ for $[-1,1]\times[-1,1]$ (i.e. the set of polynomials is formed from the tensor product of the polynomials in $x$ and $y$). Our goal is to approximate the function $f:\Omega \to \R$, with $\Omega \subset [-1,1]\times[-1,1]$. Let
\begin{align}
G_N = \text{span}\{T_k(x,y):k<N\}
\end{align}
and
\begin{align}
g_N = \argmin_{g \in G_N} \| g(x,y)-f(x,y) \|_{L^2(\Omega)}.
\end{align}
The first question we might answer is: Does $g_N \to f$ in the $L^2(\Omega)$ norm? I would think so. If there exists an extension $\varepsilon f(x,y):[-1,1]\times[-1,1] \to \R$ such that $\varepsilon f(x,y) \equiv f(x,y)$ on $\Omega$, then a straight forward convergence argument can be made. I would need to read more about this though.

Though we might could find an exact representation for $g_N$, it will either be
\begin{itemize}
\item cumbersome to compute (we would likely need to compute integrals over $\Omega$),
\item unstable to compute (as seen in the Fourier extensions).
\end{itemize}

Let $X_M \subset \Omega$ be a discrete set of $M$ points. We instead solve for

\begin{align}
\hat{g}_N = \argmin_{g \in G_N} \| \left . (g-f) \right |_{X_M} \|_2
\end{align}

The first question that comes to mind is: as $M \to \infty$, does $\hat{g}_N \to g_n$ (at least in exact arithmetic)? I would suspect so.

\subsection{Simple 2D experiment}

As a simple experiment, I try to approximate
\begin{align}
f(x,y) = \cos((x-1)^2+(y-1)^2)
\end{align}
In the region
\begin{align}
\Omega = \{(x,y) \in [-1,1] \times [-1,1]: (Ax+By+C)/B \geq 0 \}
\end{align}
i.e. the region in $[-1,1] \times [-1,1]$ above the line $Ax+By+C=0$. As a first test, I set $A=B=1$, and look at regions for $C \in [0,2]$. For $C=0$, this gives the half the square along its diagonal and $C=2$ gives the whole square. Figure~\ref{region_c} shows the region with $C=1.2$.
\begin{figure}[!h]
\centering
\includegraphics[scale = 0.5]{subset_plot.eps}
\caption{Region with $A=B=1$, $C=1.2$.}
\label{region_c}
\end{figure}

For this problem, I let
\begin{align}
G_N = \text{span}\{T_i(x) T_j(y):i,j \leq N\}
\end{align}
where $T_i(x)$ is the $i_{th}$ Chebyshev polynomial and
\begin{align}
\hat{g}_N = \argmin_{g \in G_N} \| \left . (g-f) \right |_{X} \|_2
\end{align}
for a discrete set $X \in \Omega$. For my test, I set $N=33$, and find $\hat{g}_N$ when
\begin{itemize}
\item $X$ is the subset of the $66 \times 66$ equidistant points in $\Omega$,
\item $X$ is the subset of the $66 \times 66$ Chebyshev points (i.e. a tensor product) in $\Omega$,
\item and $X$ is the subset of the $66 \times 66$ Chebyshev points in $\Omega$ with 66 equidistant points added to the diagonal boundary.
\end{itemize}
To measure the accuracy of $\hat{g}_N $ as approximation, we compute the inf norm for the $132 \times 132$ equidistant points in $\Omega$. The results can be seen in Figure~\ref{line_err_plots}. It would seem:
\begin{itemize}
\item If there is less empty space the error is lower,
\item we are better off using Chebyshev points instead of equidistant points,
\item and there is a benefit to adding points along the boundary.
\end{itemize}


\begin{figure}[!h]
\centering
\includegraphics[scale = 0.5]{line_err_plot.eps}
\caption{Semi-log plot of inf error for $C \in [0,2]$.}
\label{line_err_plots}
\end{figure}


%\subsection{Simple 1D projection}
%\begin{lemma}
%Suppose $f(x):[0,1]\to \C$ is analytic in an ellipse with foci points 0,1 and the semi minor and major axis lengths summing to $\rho/2$. Let $G_n = \text{span} \lp \{T_k(x)\}_{k=0}^N \rp$, $w(x) = \frac{1}{\sqrt{1-x^2}}$ and
%\begin{equation}
%g_n = \argmin_{g \in G_n} \left \| f(x) - g(x) \right \|_{L^2([0,1])w}.
%\end{equation}
%We then have
%\begin{equation}
%\left \| f(x) -g_n(x) \right \|_{L^2([0,1])w} \sim \rho^{-n}.
%\end{equation}
%\end{lemma}
%\begin{proof}
%Let $u(x)=2x-1$, the linear map that takes $[0,1]$ to $[-1,1]$. We have $u(x)$ maps the ellipse with foci points 0,1 and the semi minor and major axis lengths summing to $\rho/2$ to an ellipse $E_\rho$ with foci points -1,1 and the semi minor and major axis lengths summing to $\rho$. We thus have that $f(u(x))$ is analytic in $E_\rho$. Let
%\begin{equation}
%q_n = \argmin_{q \in G_n} \left \| f(u) - q(u) \right \|_{L^2([-1,1])w}.
%\end{equation}
%Since $f(u(x))$ is analytic in $E_\rho$, we have
%\begin{equation}
%\left \| f(u) - q_n(u) \right \|_{L^2([-1,1])w} \sim \rho^{-n}.
%\end{equation}
%
% We have that $G_n$ is the span of $n_{th}$ dimensional polynomials. Since 
% 
% \begin{equation}
% q_n(u(x))=q_n\lp 2x-1 \rp \in G_n
% \end{equation}
%we have
% \begin{equation}
%\left \| f(x) - g_n(x) \right \|_{L^2([0,1])w} \leq \left \| f(x) - q_n \lp 2x-1 \rp \right \|_{L^2([0,1])w} \sim \rho^{-n},
%\end{equation}
%proving the result.
%\end{proof}

\end{document}