% small.tex
\documentclass{beamer}
\usepackage{amsmath,amssymb,amsthm,color}
\usepackage{movie15}
\usetheme{Madrid}

%\usepackage[pdftex]{graphicx}
%\DeclareGraphicsExtensions{.pdf,.mps,.png,.jpg}
\usepackage{subfig}
\usepackage{algorithm,algpseudocode}
\newcommand{\R}{\mathbb{R}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Sph}{\mathbb{S}}

\newcommand{\delr}{\dfrac{r}{\delta}}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\ep}{\varepsilon}
\newcommand{\epz}{\ep\longrightarrow 0}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vf}{\vect{f}}
\newcommand{\vg}{\vect{g}}
\newcommand{\vx}{\vect{x}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vxi}{\boldsymbol{\xi}}
\newcommand{\vP}{\vect{P}}
\newcommand{\vp}{\vect{p}}
\newcommand{\vH}{\vect{H}}
\newcommand{\vI}{\vect{I}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vL}{\vect{L}}
\newcommand{\vn}{\vect{n}}
\newcommand{\vu}{\vect{u}}
\newcommand{\bfe}{\vect{e}}
\newcommand{\coef}{c}
\newcommand{\uu}{u_X}
\newcommand{\dt}{\Delta t}

\newcommand{\lam}{\lambda}
%\newcommand{\laps}{\Delta_{\mathcal{S}}}
%\newcommand{\Mlap}{\Delta_{\mathcal{S}}}
%\newcommand{\Mgrad}{\nabla_{\mathcal{S}}}
\newcommand{\laps}{\Delta_{\M}}
\newcommand{\Mlap}{\Delta_{\M}}
\newcommand{\Mgrad}{\nabla_{\M}}
\newcommand{\grad}{\nabla}
\newcommand{\MEgrad}{\vP\nabla}
\newcommand{\Mdiv}{\Mgrad\cdot}
\newcommand{\pder}[1]{\dfrac{\partial}{\partial#1}}
\newcommand{\pgradx}{\mathcal{G}^{x}}
\newcommand{\pgrady}{\mathcal{G}^{y}}
\newcommand{\pgradz}{\mathcal{G}^{z}}

\newcommand{\comment}[1]{{\color{red}{#1}}}

%Kevin's commands

\newcommand{\supp}{\operatorname{supp}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\plotwidth}{0.4}
\newcommand{\conwidth}{0.45}
\newcommand{\diffOp}{\mathcal{L}}
\newcommand{\slfrac}[2]{\left.#1\middle/#2\right.}
\DeclareMathOperator{\sech}{sech}
\newcommand{\f}{\underline{f}}
\newcommand{\vcoefs}{\underline{c}}
\newcommand{\x}{\underline{x}}
\newcommand{\y}{\underline{y}}
\newcommand{\z}{\underline{z}}

\DeclareMathOperator*{\osc}{osc}

% items enclosed in square brackets are optional; explanation below
\title[Optimal Domain Splitting]{A 2D partition of unity adaptive refinement Chebyshev polynomial method}
\author[K. Aiton]{Kevin Aiton}
\institute[UD]{
  Department of Mathematics\\
University of Delaware\\
}
\date[November 2017]{November 14, 2017}


\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% TITLE PAGE
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain]
  \titlepage
  \begin{center}
  This research was supported by National Science Foundation grant DMS-1412085
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% OUTLINE OF PRESENTATION
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[plain]{Outline of Presentation}

\begin{itemize}
	\item Overview of Chebyshev interpolant convergence theory
	\item Overview of Chebfun and Chebfun2
	\item The partition of unity method
	\item Alternating Schwarz method
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Overview of Chebyshev polynomial convergence %theory
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Overview of Chebyshev polynomial convergence theory}
We can use Chebyshev polynomials to construct approximating functions.
\begin{itemize}
\item \text{\bf Chebfun} uses polynomial interpolants based on the Chebyshev points $$x_j = \cos(j\pi/n)$$	
\item Chebyshev interpolants allow for fast and accurate algorithms forinterpolation, differentiation, root finding, etc.
\end{itemize}
\end{frame}

\begin{frame}{Overview of Chebyshev polynomial convergence theory}
Using Chebfun:	
\bigskip

\begin{center}
\includegraphics[scale = 0.4]{FishPlot.eps}
\end{center}
\begin{center}
it only took 0.03 seconds to find all these roots!	
\end{center}

\end{frame}

\begin{frame}{Overview of Chebyshev polynomial convergence theory}

Using Chebfun:	
\bigskip

\begin{center}
\includegraphics[scale = 0.4]{AiryExample.eps}
\end{center}
\begin{center}
it only took 0.02 seconds to find all these extrema points for the Airy function!	
\end{center}

\end{frame}



\begin{frame}{Overview of Chebyshev polynomial convergence theory}

The Chebyshev points allow for \textbf{spectral} convergence.

\begin{itemize}
\item If $f(x) = \sum_{i=0}^{\infty} a_i T_i(x)$, then for the interpolant $p_n(x)$ $$ \|f(x) - p_n \|_{\infty} \leq 2 \sum_{i=n+1}^{\infty} |a_i|.$$
\item For analytic functions, $|a_k|<O(\rho^{-k})$
\item Chebfun's {\tt standardChop} method uses the coefficients to determine the degree of $p_n(x)$ for a given tolerance.
\end{itemize}

\end{frame}

\begin{frame}{Chopping a Chebfun polynomial}
\begin{center}
\includegraphics[scale = 0.4]{Cheb15.eps}
\end{center}

\begin{center}
\begin{itemize}
\item \begin{center} Coefficients for $f(x)=\exp(x)/(1+10 x^2)$. \end{center}
\item \begin{center} with tol=$10^{-15}$, $n=119$ \end{center}
\item \begin{center} $\| f(x)-p_n(x) \|_{\infty}$ = 1.0e-14 \end{center}
\end{itemize} 
\end{center}
\end{frame}

\begin{frame}{Chopping a Chebfun polynomial}
\begin{center}
\includegraphics[scale = 0.4]{Cheb8.eps}
\end{center}

\begin{center}
\begin{itemize}
\item \begin{center} Coefficients for $f(x)=\exp(x)/(1+10 x^2)$. \end{center}
\item \begin{center} with tol=$10^{-8}$, $n=71$ \end{center}
\item \begin{center} $\| f(x)-p_n(x) \|_{\infty}$ = 1.1e-8 \end{center}
\end{itemize}
\end{center}
\end{frame}

\begin{frame}{2D approximation with Chebfun2}
	\textbf{Chebfun2} can approximate $f(x,y)$ with a using a low-rank approximation:
	$$
	f(x,y) \approx \sum_{j=1}^{k} \sigma_j \phi_j(y) \psi_j(x),
	$$	

	With low rank functions, we can quickly find roots, solve for extrema, integrate, differentiate , and more.

\end{frame}

\begin{frame}{2D approximation with Chebfun2}
	\begin{center}
	For example, Chebfun2 can:
	\end{center}

\begin{center}
\includegraphics[scale = 0.4]{contours.eps}
\end{center}

\begin{center}
find where $\cos(2 \pi x y) = 0.95$ in 0.14 seconds!
\end{center}

\end{frame}

\begin{frame}{2D approximation with Chebfun}

\begin{center}
It is different for high rank functions. For $f(x,y) = \arctan \lp \lp x^2+y \rp/0.01 \rp$, the rank is 815.
\end{center}
\begin{columns}

\begin{column}{0.5\textwidth}	
\begin{center}
\includegraphics[scale = 0.3]{tan2Dplot.eps}	
\end{center}
\end{column}

\begin{column}{0.5\textwidth}
Chebfun2 took approximately took 3 minutes to adaptively construct an approximation.
\end{column}

\end{columns}

\end{frame}

\begin{frame}{Refinement for 2D tensor product polynomials}
\begin{itemize}
\item We seek to do build an approximation of $f(x,y)$ with the use of tensor product approximations of the form $$ p(x,y) = \sum_{i=0}^{N_x} \sum_{j=0}^{N_y} T_i(x)T_j(y) a_{ij}$$
\end{itemize}

\begin{theorem} 
Let $f:[-1,1]^2 \to \R$ be Lipschitz continuous. Then for $\varepsilon>0$ there exists $N_x,N_y$ such that
$$
\|f(x,y)-p(x,y)\|_{\infty} \leq \varepsilon.
$$
 \end{theorem}

\end{frame}

%\begin{frame}{Refinement for 2D tensor product polynomials}
%\begin{itemize}
%\item With $$
% 	c_j(x) = \frac{2 \hat{\delta_j}}{\pi} \int_{-1}^{1} \frac{1}{\sqrt{1-y^2}}T_j(y)f(x,y)dy.
%$$ we have for fixed $\hat{x}$
%$$
%f(\hat{x},y) \approx \sum_{j=0}^{N_y}T_j(y)c_j(\hat{x})
%$$
%and for fixed $\hat{y}$
%$$
%p(x,\hat{y}) = \sum_{i=0}^{N_x} T_i(x) \frac{2 \hat{\delta_i}}{\pi} \int_{-1}^{1} \frac{1}{\sqrt{1-x^2}} \sum_{j=0}^{N_y}T_j(\hat{y})c_j(x) dx.
%$$
%
%\end{itemize}
%\end{frame}


%\begin{frame}[t]{Refinement for 2D tensor product polynomials}
%
%\begin{center}
%Key idea of proof: with $$p(x,y)=\sum_{i=0}^{N_x} T_i(x) \sum_{j=0}^{N_y} T_j(y)a_{ij}$$
%\end{center}
%
%\begin{columns}[t]
%\begin{column}[t]{0.5\textwidth}
%We can find $N_y$ such that
%$$\|f(\cdot,y) -\sum_{i=0}^{N_y} c_j(\cdot)T_j(y) \| \leq \varepsilon/2$$
%\end{column}
%\begin{column}[t]{0.5\textwidth}
%\begin{center}
%We can then find $N_x$ such that
%$$\|p(x,\cdot) - \sum_{i=0}^{N_y} c_j(x)T_j(\cdot)\|<\varepsilon/2,$$
%\end{center}	
%\end{column}
%
%\end{columns}
%
%\end{frame}


%\begin{frame}{Refinement for 2D tensor product polynomials}
%\begin{center}
%We can test $\|f(\cdot,y) -\sum_{i=0}^{N_y} c_j(\cdot)T_j(y) \| \leq \varepsilon/2$ discretely by testing the series  $\{f(x_k,y)\}$ for the Chebyshev points $\{x_k\}$.
%\end{center}
%
%\begin{center}
%For coefficients computed with FFT, we have,
%$$ c_j(x_k) = \sum_{i=0}^{N_x}T_j(x_k)a_{ij}. $$
%\end{center}
%
%\begin{center}
%We thus have
%$$ |c_j(x_k)| < \sum_{i=0}^{N_x} |a_{ij}|. $$ We use {\tt StandardChop} on $\{ \sum_{i=0}^{N_x} |a_{ij}|  \}_{j=0}^{N_y}$ to test for the condition.
%\end{center}
%
%\end{frame}
%
%\begin{frame}{Refinement for 2D tensor product polynomials}
%
%\begin{center}
%\includegraphics[scale = 0.5]{absCoeffs.eps}
%\end{center}
%
%\end{frame}
%
%\begin{frame}{Refinement for 2D tensor product polynomials}
%\begin{center}
%To test for $\|p(x,\cdot) - \sum_{i=0}^{N_y} c_j(x)T_j(\cdot)\|<\varepsilon/2$, we note: $$p(x,\cdot)=\sum_{i=0}^{N_x} T_i(x) \sum_{j=0}^{N_y} T_j(\cdot)a_{ij}$$
%and
%$$ \left | \sum_{j=0}^{N_y} T_j(\cdot)a_{ij} \right | < \sum_{j=0}^{N_y} |a_{ij}|. $$
%\end{center}
%
%\begin{center}
%We use {\tt StandardChop} on $\{ \sum_{j=0}^{N_y} |a_{ij}|  \}_{i=0}^{N_x}$ to test for the condition.
%\end{center}
%\end{frame}
%
%\begin{frame}{Refinement for 2D tensor product polynomials}
%\begin{center}
%This gives us a discrete test for convergence:
%\end{center}
%
%\begin{center}
%Run	{\tt standardChop} on $\{ \sum_{i=0}^{N_x} |a_{ij}|  \}_{j=0}^{N_y}$ and $\{ \sum_{j=0}^{N_y} |a_{ij}|  \}_{i=0}^{N_x}$.
%\end{center}
%
%\end{frame}

\begin{frame}{Partition of unity method}

To refine, we cannot just increase $N_x$ and $N_y$. $N_x N_y$ gets big!

\bigskip

\begin{itemize}
\item We instead split $[-1,1] \times [-1,1]$ into overlapping rectangles $\{ \Omega_k \}$.
\item With approximations $s_k:\Omega_k \to \R$, we define a partition of unity $w_k:\Omega \to \R$ with $\sum_k w_k=1$ so we get an approximation $$s(\vect{x}) = \sum_k w_k(\vect{x})s_k(\vect{x}).$$
\end{itemize}

\end{frame}

\begin{frame}{Partition of unity method}
\begin{center}
Some key facts:
\end{center}

\begin{itemize}
\item If each $w_k$ is smooth, and $w_k$ has support on $\Omega_k$ then $\sum_k w_k(\vect{x})s_k(\vect{x})$ is smooth.
\item Thus with overlapping subdomains, we can avoid matching!
\item With splitting, we can keep the total resolution low while still having an accurate approximation.
\end{itemize}
\end{frame}

\begin{frame}{Partition of unity method}

The domains are defined by the leaves of a kd-tree, where each split is defined by splitting the rectangle in half with a line.
\bigskip

\begin{center}
\includegraphics[scale = 0.4]{zonePlot.eps}
\end{center}
\end{frame}

\begin{frame}{Partition of unity method}

For each leaf of the tree, we form the domain by 'pudging out' the rectangle.
\bigskip

\begin{center}
\includegraphics[scale = 0.4]{domainPlot.eps}
\end{center}
\end{frame}

\begin{frame}{Partition of unity method}

We build an approximation of $f(x,y)$ on $[-1,1] \times [-1,1]$ by:
\bigskip
\begin{itemize}
\item For the polynomial approximation $s_k(x,y)=\sum_{i=0}^{N_x} T_i(x) \sum_{j=0}^{N_y} T_j(y)a_{ij}$ on $\Omega_k$, test if 
$$\|s_k(x,y) - f(x,y) \| < \varepsilon $$
via the coefficients.
\item If the do not, split $\Omega_k$ in half. Recursively repeat this on the children.
\end{itemize}

\end{frame}

\begin{frame}{Partition of unity method}

Here we adaptively build an approximation $s(x,y)$ for $f(x,y) = \arctan \lp \lp x^2+y \rp/0.01 \rp$ with tol=$10^{-12}$.
\bigskip

\begin{columns}[t]

\begin{column}{0.5\textwidth}	
\begin{center}
\includegraphics[scale = 0.3]{tan2Dplot.eps}	
\end{center}
\end{column}

\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[scale = 0.3]{tan2Dsubdomains.eps}	
\end{center}
\end{column}

\end{columns}

\begin{center}
took 2 seconds to build, $\| s(x,y)-f(x,y)\|_{\infty} = 5\times10^{-13}$.	
\end{center}

\end{frame}

\begin{frame}{BVP's}

		We are currently working on an adaptive \textbf{alternating Schwarz} method for solving boundary value problems. To solve $\mathcal{L}(u)=f$ in $\Omega$, $u=g$ on $\partial \Omega$ we:
\bigskip
	
	\begin{center}
	for $n=1,2,\dots$ solve
	\begin{equation*}
	\begin{aligned}
	&\mathcal{L}(u_1^{n+1})=f \text{ in } \Omega_1\\
	&u_1^{n+1} = g \text{ at } \partial\Omega_1 \cap \partial\Omega \\
	&u_1^{n+1} = u_2^{n} \text{ at } \partial\Omega_1 \setminus \partial\Omega 
	\end{aligned} \quad 	\begin{aligned}
	&\mathcal{L}(u_2^{n+1})=f \text{ in } \Omega_2 \\
	&u_2^{n+1} = g \text{ at } \partial\Omega_2 \cap \partial\Omega \\
	&u_2^{n+1} = u_1^{n} \text{ at } \partial\Omega_2 \setminus \partial\Omega 
	\end{aligned}
	\end{equation*}	
	\end{center}

\end{frame}

\begin{frame}{BVP's}
We are currently working on an adaptive \textbf{alternating Schwarz} method for solving boundary value problems. We develop a scheme where:

\bigskip

We use the overlapping subdomains from a kd-tree.

	\begin{center}
		\includegraphics[scale = 0.3]{ASdomain1.eps}
	\end{center}
\end{frame}

\begin{frame}{BVP's}
		We are currently working on an adaptive \textbf{alternating Schwarz} method for solving boundary value problems. We develop a scheme where:
\bigskip

		For border interpolation of $\partial \Omega_k$, we evaluate the interpolant $s_j(x,y)$ if $(x,y)$ is in the non overlapping rectangle of leaf $j$.
\bigskip

	\begin{center}
		\includegraphics[scale = 0.3]{ASdomain2.eps}
	\end{center}
\end{frame}

\begin{frame}{BVP's}
		We are currently working on an adaptive \textbf{alternating Schwarz} method for solving boundary value problems. We develop a scheme where:
\bigskip

		After obtaining a solution, split the leaves if necessary using the scheme we defined before.
\bigskip

	\begin{center}
		\includegraphics[scale = 0.3]{zonePlot2.eps}
	\end{center}
\end{frame}

\begin{frame}{BVPs}

Here we adaptively solve
$$
\varepsilon \Delta u + 2 u_x+u_y = f(x,y) \text{ in } \Omega, \quad u=b(x,y) \text{ on } \partial\Omega
$$
\bigskip
where $\varepsilon=5\times10^{-3}$ and $u(x,y) = \left(1-e^{\frac{x-1}{\epsilon }}\right) \left(1-e^{\frac{y-1}{\epsilon }}\right) \cos (\pi  (x+y)).$
\begin{columns}[t]

\begin{column}{0.5\textwidth}	
\begin{center}
\includegraphics[scale = 0.3]{coolPDE.eps}	
\end{center}
\end{column}

\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[scale = 0.3]{coolPDEdomain.eps}	
\end{center}
\end{column}

\end{columns}

\begin{center}
For this solution we adapted to tol=$10^{-8}$, and got error $5 \times 10^{-8}$.
\end{center}

\end{frame}


\begin{frame}{BVP's}
\begin{itemize}
\item We apply GMRES with alternating Schwarz as a preconditioner.
\item We currently use a two level preconditioner, where we observe the number of GMRES iterations stays constant with increasing subdomains.
\item We will take advantage of the tree to develop a multi-level preconditioner.
\end{itemize}

\end{frame}

\begin{frame}{Future Work}
\begin{itemize}
\item Adaptive solvers for nonlinear BVPs
\item Extending work into 3D and non rectangular domains
\item Apply method to tear film models with moving boundary
\end{itemize}

\end{frame}

\begin{frame}{Resources}
\begin{thebibliography}{9}
\bibitem{DriscollWeideman2014} 
Tobin A. Driscoll, and J.A.C. Weideman
\textit{Optimal Domain Splitting for Interpolation by Chebyshev Polynomials}. 
Siam Journal of Numerical Analysis, 2014
\bibitem{ApproxTref} 
Lloyd N. Trefethen,
\textit{Approximation Theory and Approximation Practice}. 
SIAM
\bibitem{cheb} 
\textit{Chebfun}. 
http://www.chebfun.org
\end{thebibliography}
\end{frame}

\end{document}